% R bootcamp, Module 9: Workflows, managing projects and good practices
% August 2013, UC Berkeley
% Chris Paciorek and (add others)

# The UNIX command line

Note: remember to start recording.

Being able to operate your computer from a command line rather than via point-and-click in a GUI environment provides incredible power and flexibility to the user, as well as allowing for reproducibility and automation in your work.

In Linux and Mac, you just need to open a Terminal window. On the Mac this is in Applications->Utilities->Terminal. Once in the Terminal, you can treat your Mac as if it is a UNIX machine (which it is). 

Windows has the old MS-DOS command line that allows some command line functionality. There is something call *cygwin* that provides UNIX-like functionality.

# Timing your code

There are a few tools in R for timing your code. 

```{r cache=TRUE}
system.time(mean(rnorm(1e7)))

library(rbenchmark)
x <- rnorm(1e7)
benchmark(ifelse(x < 0, x, 0),
                   x[x < 0] <- 0, replications = 5,
                   columns = c('replications', 'elapsed'))
```
For more advanced assessment of bottlenecks in your code, consider *Rprof()*.


# Memory use

You should know how much memory (RAM) the computer you are using has and keep in mind how big your objects are and how much memory you code might use. All objects in R are stored in RAM unlike, e.g., SAS or a database.

If in total, the jobs on a machine approach the physical RAM, the machine will start to use the hard disk as 'virtual memory'. This is called paging or swapping, and once this happens you're often toast. 

You can assess memory use with *top* or *ps* in Linux/Mac or the system monitoring tool [???] in Windows. 

```{r}
x <- rnorm(1e7)
object.size(x)
1e7*8/1e6
print(object.size(x), units = 'auto')
x <- rnorm(1e8)
gc()
rm()
gc()
```

# Debugging

As a scripting language, R essentially has a debugger working automatically. But there is an official debugger and other tools that greatly help in figuring out problems.

Let's briefly see these in action. I'll demo with a set of nested functions in which one has an error:

```{r}
fun1 <- function(x) {
     x <- x * 2
     fun2(x)
}
fun2 <- function(x) {
     fun3(x)
}
fun3 <- function(x) {
     apply(x, 1, mean)
}
```    

1) We can use *debug()* to step through a function line by line
2) After an error occurs, we can use traceback() to look at the *call stack*
3) More helpfully, if we set `options(error = recover)` before running code, we can go into the function in which the error occurred
4) We can insert `browser()` inside a function and R will stop there and allow us to proceed with debugging statements
5) You can temporarily insert code into a function (including built-in functions) with `trace(fxnName, edit = TRUE)`

# Scripting

Keeping all your code in script (i.e., text) files is the way to go. Try to keep your files modular and focused. 

If you find yourself writing code to do something multiple times, write a function to do it and just call that function. 

If you use a good editor (such as RStudio's built-in editor, emacs with ESS, Aquamacs, [.... - see notes from intro session for new students]), it's easier to write and understand your code.

You can then run blocks of code within RStudio easily.

To run all the code in an entire file, do `source('myCodeFile.R')`.

To run code as a background job in a Linux/Mac context:
`R CMD BATCH --no-save myCodeFile.R myOutput.Rout &`

Then you don't need to leave RStudio or R or a terminal window open. Your job will run by itself. If you are logged into a remote machine, you can log out and come back later. 

IMPORTANT: make sure to write any needed output to a file (e.g. .Rda files, CSV files, text output from print() or cat()).

# Version control

The basic idea is that instead of manually trying to keep track of what changes you've made to code, data, documents, you use software to help you manage the process. This has several benefits:

* easily allowing you to go back to earlier versions
* allowing you to have multiple version you can switch between
* allowing you to share work easily without worrying about conflicts
* providing built-in backup

A popular tool these days is *git*, with storage for projects at github.com. 

At a basic level, a simple principle is to have version numbers for all your work: code, datasets, manuscripts. Whenever you make a change to a dataset, increment the version number. For code and manuscripts, increment when you make substantial changes or have obvious breakpoints in your workflow. 

# Some ideas about reproducibility

* Never change a dataset manually. Always have a script that operates on the data. 
* Note when and where you download, and ideally download via a script as well (e.g., wget and curl are Linux tools for automated downloading.
* Produce figures (e.g., from R) via a script and not by point-and-click
* When making figures, use *save()* or *save.image()* to save all the inputs needed to recreate a figure, with the code for making the figure in a script file
* If feasible, include your code for doing analyses and making figures in the relevant document reporting the work by using Rmarkdown or Latex with knitr or Sweave. 
* Set the random number seed so someone else can duplicate your exact numbers

# Coding practices

Try to avoid using the names of standard R functions for your objects, but R will generally be fairly smart about things.

```{r}
c <- 7
c(3,5)
c
rm(c)
c
```

Try to use upperCamelCase or underscores when naming your objects (and don't mix and match). Leave periods for use in the context of S3 objects. 

```{r}
precipExtremes <- rnorm(5)  # precip_extremes
timeInDays <- rnorm(5)   # time_in_days
summary.lm(lm(precipExtremes ~ timeInDays))
```
