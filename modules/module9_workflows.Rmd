% R bootcamp, Module 9: Workflows, managing projects and good practices
% August 2013, UC Berkeley
% Jarrod Millman and Chris Paciorek

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
```

# Outline

Note: remember to start recording.

This module is less focused on R. More to do with general recommendations on
best practices when working on a computer. Much of the discussion will apply
to other programming languages such as Python as well as writing papers,
posters, slides, etc. with tools such as $\LaTeX$.

General themes:

* Automation
* Toolbox
* Best practices

# Making dirt fly!

![](figs/panama-canal.png)

# Scripting

Keeping all your code in script (i.e., text) files is the way to go. Try to keep your files modular and focused.

If you find yourself writing code to do something multiple times, write a function to do it and just call that function.

If you use a good editor (such as RStudio's built-in editor, emacs with ESS, Aquamacs, [.... - see notes from intro session for new students]), it's easier to write and understand your code.

You can then run blocks of code within RStudio easily.

To run all the code in an entire file, do `source('myCodeFile.R')`.

To run code as a background job in a Linux/Mac context:
`R CMD BATCH --no-save myCodeFile.R myOutput.Rout &`

Then you don't need to leave RStudio or R or a terminal window open. Your job will run by itself. If you are logged into a remote machine, you can log out and come back later.

IMPORTANT: make sure to write any needed output to a file (e.g. .Rda files, CSV files, text output from print() or cat()).

# Debugging

As a scripting language, R essentially has a debugger working automatically. But there is an official debugger and other tools that greatly help in figuring out problems.

Let's briefly see these in action. I'll demo with a set of nested functions in which one has an error:

```{r}
fun1 <- function(x) {
     x <- x * 2
     fun2(x)
}
fun2 <- function(x) {
     fun3(x)
}
fun3 <- function(x) {
     apply(x, 1, mean)
}
```    

1) We can use *debug()* to step through a function line by line
2) After an error occurs, we can use traceback() to look at the *call stack*
3) More helpfully, if we set `options(error = recover)` before running code, we can go into the function in which the error occurred
4) We can insert `browser()` inside a function and R will stop there and allow us to proceed with debugging statements
5) You can temporarily insert code into a function (including built-in functions) with `trace(fxnName, edit = TRUE)`



# Timing your code

There are a few tools in R for timing your code.

```{r cache=TRUE}
system.time(mean(rnorm(1e7)))

library(rbenchmark)
x <- rnorm(1e7)
benchmark(ifelse(x < 0, x, 0),
                   x[x < 0] <- 0, replications = 5,
                   columns = c('replications', 'elapsed'))
```
For more advanced assessment of bottlenecks in your code, consider *Rprof()*.


# Memory use

You should know how much memory (RAM) the computer you are using has and keep in mind how big your objects are and how much memory you code might use. All objects in R are stored in RAM unlike, e.g., SAS or a database.

If in total, the jobs on a machine approach the physical RAM, the machine will start to use the hard disk as 'virtual memory'. This is called paging or swapping, and once this happens you're often toast.

You can assess memory use with *top* or *ps* in Linux/Mac or the system monitoring tool [???] in Windows.

```{r}
x <- rnorm(1e7)
object.size(x)
1e7*8/1e6
print(object.size(x), units = 'auto')
x <- rnorm(1e8)
gc()
rm()
gc()
```

# The UNIX command line

Being able to operate your computer from a command line rather than via point-and-click in a GUI environment provides incredible power and flexibility to the user, as well as allowing for reproducibility and automation in your work.

In Linux and Mac, you just need to open a Terminal window. On the Mac this is in Applications->Utilities->Terminal. Once in the Terminal, you can treat your Mac as if it is a UNIX machine (which it is).

Windows has the old MS-DOS command line that allows some command line functionality. There is something call *cygwin* that provides UNIX-like functionality.

# The UNIX Philosophy

> Write programs that do one thing and do it well.
>
> Write programs to work together.
>
> Write programs to handle text streams, because that is a universal interface.

--- Doug McIlroy

For more details, please see:

http://en.wikipedia.org/wiki/Unix_philosophy




# Reproducible research

> An article about computational science in a scientific
publication is **not the scholarship itself**, it is merely
**advertising** of the scholarship. The actual scholarship is the
complete software development environment and the
complete set of instructions which generated the figures.

--- Jonathan Buckheit and David Donoho, WaveLab and
Reproducible Research (1995)

# Version control

The basic idea is that instead of manually trying to keep track of what changes you've made to code, data, documents, you use software to help you manage the process. This has several benefits:

* easily allowing you to go back to earlier versions
* allowing you to have multiple version you can switch between
* allowing you to share work easily without worrying about conflicts
* providing built-in backup

At a basic level, a simple principle is to have version numbers for all your work: code, datasets, manuscripts. Whenever you make a change to a dataset, increment the version number. For code and manuscripts, increment when you make substantial changes or have obvious breakpoints in your workflow. 

# Version control architectures

* Client-server (e.g., CVS, Subversion)
* Distributed (e.g., Mercurial, **Git**)

# Git data structure



# Git concepts: commit

a **snapshot** of work at a point in time

![Credit: ProGit book, by Scott Chacon, CC License.](figs/commit_anatomy.png)

# Git concepts: repository

a group of **linked** commits (DAG)

![Credit: ProGit book, by Scott Chacon, CC License.](figs/threecommits.png)

# Git concepts: hash

toy "implementation"

```{r}
library('digest')

# first commit
data1 <- 'This is the start of my paper2.'
meta1 <- 'date: 8/20/13'
hash1 = digest(c(data1,meta1), algo="sha1")
cat('Hash:', hash1)

# second commit, linked to the first
data2 <- 'Some more text in my paper...'
meta2 <- 'date: 8/20/13'
# Note we add the parent hash here!
hash2 <- digest(c(data2,meta2,hash1), algo="sha1")
cat('Hash:', hash2)
```

# Git

```{r test-bash, engine='bash'}
git help
```

# Learning Git

* [Git for Scientists: A Tutorial](http://nyuccl.org/pages/GitTutorial/)
* [Gitwash: workflow for scientific Python projects](http://matthew-brett.github.io/pydagogue/gitwash_build.html)
* [Git branching demo](http://pcottle.github.io/learnGitBranching/)

# Some ideas about reproducibility

* Never change a dataset manually. Always have a script that operates on the data. 
* Note when and where you download, and ideally download via a script as well (e.g., wget and curl are Linux tools for automated downloading.
* Produce figures (e.g., from R) via a script and not by point-and-click
* When making figures, use *save()* or *save.image()* to save all the inputs needed to recreate a figure, with the code for making the figure in a script file
* If feasible, include your code for doing analyses and making figures in the relevant document reporting the work by using Rmarkdown or Latex with knitr or Sweave. 
* Set the random number seed so someone else can duplicate your exact numbers

# Github

![](figs/ipython_github_pull1.png)

# knitr

![](figs/knitr-book.png)

# Implementing Reproducible Computational Research

![In many of today’s research fields, including biomedicine, computational tools are increasingly being used so that the results can be reproduced. Researchers are now encouraged to incorporate software, data, and code in their academic papers so that others can replicate their research results. Edited by three pioneers in this emerging area, this book is the first one to explore this groundbreaking topic. It presents various computational tools useful in research, including cloud computing, data repositories, virtual machines, R’s Sweave function, XML-based programming, and more. It also discusses legal issues and case studies.](figs/ircr-book.png)

# Best practices

![[arXiv:1210.0530v3 [cs.MS] 29 Nov 2012](http://arxiv.org/abs/1210.0530v3)](figs/best-practices.png)

# Coding practices

Try to avoid using the names of standard R functions for your objects, but R will generally be fairly smart about things.

```{r}
c <- 7
c(3,5)
c
rm(c)
c
```

Try to use upperCamelCase or underscores when naming your objects (and don't mix and match). Leave periods for use in the context of S3 objects. 

```{r}
precipExtremes <- rnorm(5)  # precip_extremes
timeInDays <- rnorm(5)   # time_in_days
summary.lm(lm(precipExtremes ~ timeInDays))
```

# Breakout
