% R bootcamp, Module 7: Core tools
% August 2013, UC Berkeley
% Chris Paciorek and (add others)

# Optimization

Note: remember to start recording.

R provides functionality for optimization - finding maxima or minima of a function. 

A workhorse is `optim()`, which implements a number of optimization algorithms. 

```{r}
 banana <- function(x) {   ## Rosenbrock Banana function
         x1 <- x[1]
         x2 <- x[2]
         100 * (x2 - x1 * x1)^2 + (1 - x1)^2
     }

x1s <- x2s <- seq(-5, 5, length = 100)
x <- expand.grid(x1s, x2s)
fx <- apply(x, 1, banana)

par(mfrow = c(1, 2))
image.plot(x1s, x2s, matrix(fx, 100))
image.plot(x1s, x2s, matrix(log(fx), 100))

optim(c(-2,0), banana)
 
if(FALSE) {
banana <- function(x) {   ## Rosenbrock Banana function
         points(x[1],x[2])
         x1 <- x[1]
         x2 <- x[2]
         100 * (x2 - x1 * x1)^2 + (1 - x1)^2
     }
optim(c(-2,0), banana)
}
```

# Smoothing

Linear regression and GLMs are of course useful, but often the relationship is not linear, even on some transformed scale.

Additive models and generalized additive models (GAMs) are the more flexible variants on linear models and GLMs.

There are a variety of tools in R for modeling nonlinear and smooth relationships, mirroring the variety of methods in the literature.

One workhorse is `gam()` in the *mgcv* package.

# GAM in action

Let's consider height in the earnings dataset.

Any hypotheses about the relationship of earnings with height and education?

```{r}
data <- read.dta('../data/heights.dta')
mod_male <- gam(earn ~ s(height, k = 10) + s(ed, k = 10), data = data[data$sex == 1,])
mod_female <- gam(earn ~ s(height, k = 10) + s(ed, k = 10), data = data[data$sex == 2,])
summary(mod_male)
summary(mod_female)
# plot(mod_male)
# plot(mod_female)
```

# A bit more model-building 

Suppose we want to account for race in the model.

```{r}
data <- read.dta('../data/heights.dta')
mod_male <- gam(earn ~ s(height, k = 10) + s(ed, k = 10) + race, data = data[data$sex == 1,])
mod_female <- gam(earn ~ s(height, k = 10) + s(ed, k = 10) + race, data = data[data$sex == 2,])
summary(mod_male)
summary(mod_female)
# plot(mod_male)
# plot(mod_female)
```

Comments? 


# Mixed effects models

should I cover this?

# Distributions
Since R was developed by statisticians, it handles distributions and simulation seamlessly.

All commonly-used distributions have functions in R. Each distribution has a family of functions: 

* d - probability density/mass function
* r - generate a random value 
* p - cumulative distribution function
* q - quantile function (inverse CDF)

Some of the distributions include the following (in the form of their random number generator function): `dnorm()`, `dunif()`, `dbinom()`, `dpois()`, `dbeta()`, `dgamma()`, dt

# The normal distribution in action

```{r}
pnorm(1.96)
qnorm(.975)
dbinom(0:10, size = 10, prob = 0.3)
dnorm(5)
dt(5, df = 1)
rmultinom(1, 100, prob = c(.1, .1, .2, .3, .25, .05)) 

x <- seq(-5, 5, length = 100)
plot(x, dnorm(x), type = 'l')
lines(x, dt(x, df = 1), col = 'red')
```

# Other types of simulation and sampling

We can draw a sample with or without replacement. Here's an example that starts to lead to a bootstrap.

```{r}
sample(row.names(state.x77), 7, replace = FALSE)

require(foreign)
data <- read.dta('../data/heights.dta')

mean(data$earn, na.rm = TRUE)
smp <- sample(seq_len(nrow(data)), replace = TRUE)
mean(data$earn[smp], na.rm = TRUE)
```

# The Random Seed

A few key facts about generating random numbers

* Random number generation is based on generating uniformly between 0 and 1 and the and then transforming to the kind of random number of interest
* Random numbers on a computer are pseudo-random; they are generated deterministically from a very, very, very long sequence that repeats
* The seed determines where you are in that sequence

To replicate any work involving random numbers, make sure to set the seed first.

```{r}
set.seed(0)
vals <- rnorm(10)
vals
vals <- rnorm(10)
vals
set.seed(0)
vals <- rnorm(10)
vals
```

# Dates
- R has built-in ways to handle dates (don't reinvent the wheel!) 

```{r dates}
date1 <- as.Date("03-01-2011", format = "%m-%d-%Y")
date2 <- as.Date("03/02/11", format = "%m/%d/%y")
date3 <- as.Date("07-May-11", format = "%d-%b-%y")
class(date1)
dates <- c(date1, date2, date3)
weekdays(dates)
dates + 30
date3 - date2
unclass(dates)
```
- The origin date in R is January 1, 1970


# Time too!

```{r}
library(chron)
d1 <- chron("12/25/2004", "10:37:59") # default format of m/d/Y and h:m:s
d2 <- chron("12/26/2004", "11:37:59")
d1
d1 + 33
d2 - d1
d1 + d2
```

There's lots more packages/functionality for dates/times: see *lubridate*, `?DateTimeClasses`
 
# Breakout 

Write code to do 10-fold cross-validation on an analysis of [some data]. First you need to randomly split your dataset into 10 equal-size chunks. Then hold out a single chunk in turn, fitting with the remaining and predicting for the remaining chunks.
